# Topic

## Papers

| Title                                                        | Year | Authors                     |
| ------------------------------------------------------------ | ---- | --------------------------- |
| Intriguing properties of neural networks                     | 2013 | Christian Szegedy et al.    |
| Explaining and harnessing adversarial examples               | 2014 | Ian Goodfellow et al.       |
| Going deeper with convolutions                               | 2015 | Christian Szegedy et al.    |
| Adversarial machine learning at scale                        | 2016 | Alexey Kurakin et al.       |
| Practical black-box attacks against deep learning systems using adversarial examples | 2016 | Nicolas Papernot et al.     |
| The limitations of deep learning in adversarial settings     | 2016 | Nicolas Papernot et al.     |
| Distillation as a defense to adversarial perturbations against deep neural networks | 2016 | Nicolas Papernot et al.     |
| Towards deep learning models resistant to adversarial attacks | 2017 | Aleksander Madry et al.     |
| Towards evaluating the robustness of neural networks         | 2017 | Nicholas Carlini et al.     |
| The Space of Transferable Adversarial Examples               | 2017 | Florian Tram{\`{e}}r et al. |
| Ensemble Adversarial Training: Attacks and Defenses          | 2017 | Florian Tram{\`{e}}r et al. |
| Houdini: Fooling Deep Structured Prediction Models           | 2017 | Moustapha Cisse et al.      |
| Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples | 2018 | Anish Athalye et al.        |
| Adversarial Spheres                                          | 2018 | Justin Gilmer et al.        |

## Reading List

## code

## datasets

## software